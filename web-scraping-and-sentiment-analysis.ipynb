{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5635391,"sourceType":"datasetVersion","datasetId":3240077}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Classification and Analysis On Extracted Data from website articles.","metadata":{"id":"w0lKqk3HZol3"}},{"cell_type":"markdown","source":"# Importing Data","metadata":{"id":"d4enqqo-aJ8P"}},{"cell_type":"code","source":"import pandas as pd\n\ninput_data = pd.read_excel('/kaggle/input/website-urls-for-web-scraping-and-data-analysis/blackcoffer websites link dataset.xlsx')\nprint(input_data)","metadata":{"id":"Pqz83onDDO_2","outputId":"9c65b57e-8c24-4b2f-f3b4-a21f74708182","execution":{"iopub.status.busy":"2023-05-08T17:42:02.750451Z","iopub.execute_input":"2023-05-08T17:42:02.750878Z","iopub.status.idle":"2023-05-08T17:42:03.358779Z","shell.execute_reply.started":"2023-05-08T17:42:02.750839Z","shell.execute_reply":"2023-05-08T17:42:03.355849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Web Scraping\n\nCreating a function to extract article title and text content using url of article","metadata":{"id":"lhHFMDQoDgsM"}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\ndef extract_data_in_txt(url, url_id):\n\n  # Making a GET request for the given article link\n  r = requests.get(url)\n\n  # Parsing the HTML\n  soup = BeautifulSoup(r.content, 'html.parser')\n  \n  # Finding the title and all div elements to get article contents (text only)\n  title = soup.find('title')\n  div = soup.find_all('div',  class_ = 'td-post-content tagdiv-type') \n\n  # Creating a text file with particular url's id as file name\n  filename = str(url_id)\n  file =  open(filename + \".txt\", \"w\")  #Opening file in write mode\n\n  \n  file.write(title.text.strip())  # Removing extra spaces from the title and convertion in lower case before writing in file\n  file.write(\".\")\n\n  for i in div:\n    lines = i.find_all('p') #finding all 'p: paragraph' type attributes of article\n\n    # extracting text data line by line and writing into a file\n    for line in lines:\n      file.write(line.text.strip())  # Removing extra spaces from the text before writing in file\n      file.write(\".\")\n\n  file.close() # Close the file after writing all the text content of an article\n\n#example\nextract_data_in_txt(input_data.URL[0], input_data.URL_ID[0])\nfilename = str(input_data.URL_ID[0])\nfile =  open(filename + \".txt\", \"r\") \nprint(file.read())","metadata":{"id":"fUXRWWvlQ68M","outputId":"aac1e312-78ac-4e30-d990-2796161d2220","execution":{"iopub.status.busy":"2023-05-08T17:42:03.360499Z","iopub.execute_input":"2023-05-08T17:42:03.361046Z","iopub.status.idle":"2023-05-08T17:42:03.899232Z","shell.execute_reply.started":"2023-05-08T17:42:03.361015Z","shell.execute_reply":"2023-05-08T17:42:03.898318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating text files for each article \nEach article's title and content is stored in a text file as url_id its filename.","metadata":{"id":"o3vWDfGYblIZ"}},{"cell_type":"code","source":"def create_files():\n  for i in range(len(input_data.URL)):\n    extract_data_in_txt(input_data.URL[i], input_data.URL_ID[i])\n\ncreate_files()","metadata":{"id":"ZLLkW_gxRe0y","execution":{"iopub.status.busy":"2023-05-08T17:42:03.900507Z","iopub.execute_input":"2023-05-08T17:42:03.901086Z","iopub.status.idle":"2023-05-08T17:43:30.784941Z","shell.execute_reply.started":"2023-05-08T17:42:03.901054Z","shell.execute_reply":"2023-05-08T17:43:30.78374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis","metadata":{"id":"BZGYk13QOVra"}},{"cell_type":"markdown","source":"# Creating a list of stop words","metadata":{"id":"5na0a-NFy4W0"}},{"cell_type":"code","source":"import re\n\n#filenames for all files containing stop words\npath = \"/kaggle/input/website-urls-for-web-scraping-and-data-analysis/StopWords_\"\n\nauditor = path + \"Auditor.txt\"\ncurrency = path + \"Currencies.txt\"\nnumbers = path + \"DatesandNumbers.txt\"\ngeneric = path + \"Generic.txt\"\ngeneric_long = path + \"GenericLong.txt\"\ngeographic = path + \"Geographic.txt\"\nnames = path + \"Names.txt\"\n\nfilenames = [auditor, currency, numbers, generic, generic_long, geographic, names]\n\n# Create a list to contain all the stop words\nstop_words = []\n\nfor fname in filenames:\n    df = pd.read_csv(fname, sep = \"|\", encoding='latin-1', header = None)\n    word_list = [str(i).lower().strip() for i in df[0]]\n    stop_words.extend(word_list)\n    \n# printing all the stop words\nprint(stop_words)","metadata":{"id":"FmQCIOdCyOS3","outputId":"412895db-ec34-4be6-89a3-0173640bcbcc","execution":{"iopub.status.busy":"2023-05-08T17:43:30.787828Z","iopub.execute_input":"2023-05-08T17:43:30.788298Z","iopub.status.idle":"2023-05-08T17:43:30.879664Z","shell.execute_reply.started":"2023-05-08T17:43:30.788248Z","shell.execute_reply":"2023-05-08T17:43:30.878794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a dictionary of positive and negative words","metadata":{"id":"ffaagLEFJU7j"}},{"cell_type":"code","source":"path = \"/kaggle/input/website-urls-for-web-scraping-and-data-analysis/\"\n\npositive = path + \"positive-words.txt\"\nnegative = path + \"negative-words.txt\"\n\n# Create lists for positive and negative words\npos_words = []\nneg_words = []\n\ndf = pd.read_csv(positive, sep = \" \", encoding='latin-1', header = None)\npos_words = [str(i).lower().strip() for i in df[0] if(str(i).lower().strip() not in stop_words)]\n    \n\ndf = pd.read_csv(negative, sep = \" \", encoding='latin-1', header = None)\nneg_words = [str(i).lower().strip() for i in df[0] if(str(i).lower().strip() not in stop_words)]\n\ndictionary = {'positive': pos_words,\n                      'negative': neg_words}\n\n# printing all the positive and negative words\nprint(dictionary)","metadata":{"id":"YpDDVNf_y2pW","outputId":"2e9bcdb9-e962-4b79-b35f-f7d663579e7a","execution":{"iopub.status.busy":"2023-05-08T17:43:30.880893Z","iopub.execute_input":"2023-05-08T17:43:30.881352Z","iopub.status.idle":"2023-05-08T17:43:32.24305Z","shell.execute_reply.started":"2023-05-08T17:43:30.881324Z","shell.execute_reply":"2023-05-08T17:43:32.241816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{"id":"PmuN8Pi7LSEj"}},{"cell_type":"code","source":"!pip install contractions","metadata":{"id":"jq4kN769NMFg","outputId":"17589620-654b-4c67-97f1-e65dc10a6e97","execution":{"iopub.status.busy":"2023-05-08T17:43:32.244546Z","iopub.execute_input":"2023-05-08T17:43:32.244975Z","iopub.status.idle":"2023-05-08T17:43:44.824885Z","shell.execute_reply.started":"2023-05-08T17:43:32.244937Z","shell.execute_reply":"2023-05-08T17:43:44.823312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Expanding contractions","metadata":{"id":"Q2oz4JS7_BR1"}},{"cell_type":"code","source":"import contractions\n\n#Expanding contractions such as I'll to I will etc.\ndef expand_contractions(s):\n    return contractions.fix(s)","metadata":{"id":"3z9UNskbESZ2","execution":{"iopub.status.busy":"2023-05-08T17:43:44.82691Z","iopub.execute_input":"2023-05-08T17:43:44.827254Z","iopub.status.idle":"2023-05-08T17:43:44.846326Z","shell.execute_reply.started":"2023-05-08T17:43:44.82722Z","shell.execute_reply":"2023-05-08T17:43:44.845406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatizing sentence: converting words in their lemma(root) form","metadata":{"id":"2vCVNTL3_AeO"}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\n \n# Create a Doc object\ndef lemmatize_sentence(sentence):\n    doc = nlp(sentence)\n    # Create list of tokens from given string\n    tokens = []\n    for token in doc:\n        tokens.append(token)\n \n    lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n    return lemmatized_sentence ","metadata":{"id":"6z42jLEDNKvq","outputId":"319c14f8-3dfb-4d90-f050-b0df288c72a0","execution":{"iopub.status.busy":"2023-05-08T17:43:44.847676Z","iopub.execute_input":"2023-05-08T17:43:44.848571Z","iopub.status.idle":"2023-05-08T17:43:58.828157Z","shell.execute_reply.started":"2023-05-08T17:43:44.848532Z","shell.execute_reply":"2023-05-08T17:43:58.826891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_data(clean_sentence):\n  \n  clean_sentence = expand_contractions(clean_sentence)\n\n  clean_sentence = re.sub(r'[^\\w\\s]', '', clean_sentence)  #removing punctuations like \"/\", \";\" \"[\", \"]\" \"=\", \"#\" etc.\n\n  clean_sentence = lemmatize_sentence(clean_sentence)\n\n  return clean_sentence","metadata":{"id":"RVpLmMD1ZA-r","execution":{"iopub.status.busy":"2023-05-08T17:43:58.829478Z","iopub.execute_input":"2023-05-08T17:43:58.830116Z","iopub.status.idle":"2023-05-08T17:43:58.835067Z","shell.execute_reply.started":"2023-05-08T17:43:58.830084Z","shell.execute_reply":"2023-05-08T17:43:58.8342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Derived variables\n\nWe convert the text into a list of tokens using the nltk tokenize module and use these tokens to calculate the 4 variables described below:\n\n1. **Positive Score**: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n2. **Negative Score**: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n3. **Polarity Score**: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \nPolarity Score = (Positive Score â€“ Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\nRange is from -1 to +1\n4. **Subjectivity Score**: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula:Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001). Range is from 0 to +1 \n","metadata":{"id":"aFFHdtSp_0-H"}},{"cell_type":"code","source":"from nltk import word_tokenize\n\ndef extracting_derived_scores(clean_sentence):\n\n  clean_sentence = clean_sentence.lower() \n\n  clean_sentence = clean_data(clean_sentence)\n\n  clean_sentence = clean_sentence.split(\" \")  #tokenization: splitting sentence into words\n\n  clean_lst = []  #removing stop words like if, but, or etc. and removing characters of length 1\n\n  for word in clean_sentence:\n    if (word not in stop_words):\n      clean_lst.append(word)\n\n  positivity_score = 0\n  negativity_score = 0\n\n  for word in clean_lst:\n    if(word in pos_words):\n      positivity_score += 1\n    if(word in neg_words):\n      negativity_score += 1\n    else:\n      continue\n\n  polarity_score = (positivity_score - negativity_score) / ((positivity_score + negativity_score) + 0.000001)\n  subjectivity_score = (positivity_score + negativity_score) / ((len(clean_lst)) + 0.000001)\n\n  scores = [positivity_score, negativity_score, polarity_score, subjectivity_score]\n\n  return scores\n","metadata":{"id":"uaVig8mFOBNb","execution":{"iopub.status.busy":"2023-05-08T17:43:58.838655Z","iopub.execute_input":"2023-05-08T17:43:58.839764Z","iopub.status.idle":"2023-05-08T17:44:00.122507Z","shell.execute_reply.started":"2023-05-08T17:43:58.839714Z","shell.execute_reply":"2023-05-08T17:44:00.121174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a list filenames, containing filename of every article\n\npath = \"/kaggle/working/\"\n\nfilenames = [path+str(i)+\".txt\" for i in input_data.URL_ID]","metadata":{"id":"REBzgcxgSFYc","execution":{"iopub.status.busy":"2023-05-08T17:44:00.127148Z","iopub.execute_input":"2023-05-08T17:44:00.127499Z","iopub.status.idle":"2023-05-08T17:44:00.133011Z","shell.execute_reply.started":"2023-05-08T17:44:00.127471Z","shell.execute_reply":"2023-05-08T17:44:00.13176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#storing these 4 scores in seperate lists\n#each list contains that particular score for all the text files.\n\npositivity_score = [] \nnegativity_score = [] \npolarity_score = [] \nsubjectivity_score = []\n\nfor fname in filenames:\n  file = open(fname, 'r')\n  text = file.read()\n  score = extracting_derived_scores(text)\n  positivity_score.append(score[0])\n  negativity_score.append(score[1])\n  polarity_score.append(score[2])\n  subjectivity_score.append(score[3])\n","metadata":{"id":"7gr7hpiSErJG","execution":{"iopub.status.busy":"2023-05-08T17:44:00.134605Z","iopub.execute_input":"2023-05-08T17:44:00.13497Z","iopub.status.idle":"2023-05-08T17:44:34.330813Z","shell.execute_reply.started":"2023-05-08T17:44:00.134941Z","shell.execute_reply":"2023-05-08T17:44:34.329613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average Sentence Length\n\nSum of sentence lengths / Total no. of sentences","metadata":{"id":"Eq3mr6AARCM0"}},{"cell_type":"code","source":"avg_sentence_len = []  #list to score avg sentence length of each document\n\nfor fname in filenames:\n\n  file = open(fname, 'r')\n  text = file.read() #extracting all the file content in a string\n\n  sentence_list = text.split(\".\") #split function will return a list containting sentences seperated by full-stop\n  \n  sentence_lengths = [len(i) for i in sentence_list]\n\n  sentence_count = len(sentence_list)\n\n  avg_sentence_len.append(sum(sentence_lengths) / sentence_count)\n","metadata":{"id":"tBvoNK4MIxov","execution":{"iopub.status.busy":"2023-05-08T17:44:34.332203Z","iopub.execute_input":"2023-05-08T17:44:34.332597Z","iopub.status.idle":"2023-05-08T17:44:34.351791Z","shell.execute_reply.started":"2023-05-08T17:44:34.332558Z","shell.execute_reply":"2023-05-08T17:44:34.350575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average No. of Words per Sentence\n\nSum of no. of words in a sentence / Total no. of sentences","metadata":{"id":"ZhqZoG0dRErO"}},{"cell_type":"code","source":"avg_words_per_sentence = [] #list to score avg no. of words per sentence for each document\n\nfor fname in filenames:\n\n  words_in_sentence_count = []\n\n  file = open(fname, 'r')\n  text = file.read() #extracting all the file content in a string\n\n  sentence_list = text.split(\".\") #split function will return a list containting sentences seperated by full-stop\n  \n  for i in sentence_list:\n    word_count = 0\n    for word in i.split(\" \"):\n      word_count += 1\n    words_in_sentence_count.append(word_count)\n  \n  sentence_count = len(sentence_list)\n\n  avg_words_per_sentence.append(sum(words_in_sentence_count) / sentence_count)\n","metadata":{"id":"nN_7b4RDRXgr","execution":{"iopub.status.busy":"2023-05-08T17:44:34.353372Z","iopub.execute_input":"2023-05-08T17:44:34.354378Z","iopub.status.idle":"2023-05-08T17:44:34.392655Z","shell.execute_reply.started":"2023-05-08T17:44:34.354342Z","shell.execute_reply":"2023-05-08T17:44:34.391511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average Word Length\n\nSum of lengths of words / total words in a document","metadata":{"id":"phy82KMMRKB9"}},{"cell_type":"code","source":"avg_words_length = [] #list to score avg word length of each document\n\nfor fname in filenames:\n  \n  word_lengths = []\n\n  file = open(fname, 'r')\n  text = file.read()\n\n  text = re.sub(r'\\.', '', text)\n  \n  text = text.split(\" \")\n\n  for i in text:\n    word_lengths.append(len(i))\n    \n  avg_words_length.append(sum(word_lengths) / len(text))","metadata":{"id":"VZzAGKq7doFO","execution":{"iopub.status.busy":"2023-05-08T17:44:34.394017Z","iopub.execute_input":"2023-05-08T17:44:34.394311Z","iopub.status.idle":"2023-05-08T17:44:34.440159Z","shell.execute_reply.started":"2023-05-08T17:44:34.394286Z","shell.execute_reply":"2023-05-08T17:44:34.439258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Count after Data Cleaning","metadata":{"id":"B-aDBbGZWo2m"}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"id":"_Us3DlKOdCDR","outputId":"fabb8aa0-cff3-4bab-e9e6-ee428c4c8b1f","execution":{"iopub.status.busy":"2023-05-08T17:44:34.441405Z","iopub.execute_input":"2023-05-08T17:44:34.441906Z","iopub.status.idle":"2023-05-08T17:44:34.524538Z","shell.execute_reply.started":"2023-05-08T17:44:34.441877Z","shell.execute_reply":"2023-05-08T17:44:34.523439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport re\n\nword_count = []\n\nfor fname in filenames:\n\n  file = open(fname, 'r')\n  text = file.read()\n    \n  text = re.sub(r'[^\\w\\s]', '', text)  #removing punctuations like \"/\", \";\" \"[\", \"]\" \"=\", \"#\" etc.       \n  text = text.split()  #tokenization: splitting sentence into words\n    \n  stop_words = set(stopwords.words('english'))\n  clean_lst = []  #removing stop words like if, but, or etc. and removing characters of length 1\n  for word in text:\n      if (word not in stop_words):\n          if(len(word) != 1):\n              clean_lst.append(word)\n              \n  word_count.append(len(clean_lst))","metadata":{"id":"rjZY_kczX6Jp","execution":{"iopub.status.busy":"2023-05-08T17:44:34.525609Z","iopub.execute_input":"2023-05-08T17:44:34.525952Z","iopub.status.idle":"2023-05-08T17:44:34.615023Z","shell.execute_reply.started":"2023-05-08T17:44:34.525924Z","shell.execute_reply":"2023-05-08T17:44:34.613913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Personal Pronouns","metadata":{"id":"FB2WhdLlYe5Z"}},{"cell_type":"code","source":"personal_pronouns = re.compile(r'\\b(I|we|ours|my|mine|(?-i:us))\\b', re.I)\npronoun_count = [] #list to contain the no. of personal pronouns in each document\n\nfor fname in filenames:\n\n  file = open(fname, 'r')\n  text = file.read()\n    \n  pronouns = personal_pronouns.findall(text) \n\n  pronoun_count.append(len(pronouns))\n ","metadata":{"id":"KHgDQzz3YeNG","execution":{"iopub.status.busy":"2023-05-08T17:44:34.616266Z","iopub.execute_input":"2023-05-08T17:44:34.616596Z","iopub.status.idle":"2023-05-08T17:44:34.666903Z","shell.execute_reply.started":"2023-05-08T17:44:34.616568Z","shell.execute_reply":"2023-05-08T17:44:34.666001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average Syllable Count per Word\n\nSum of syllable count in each word / total no. of words","metadata":{"id":"quxqtzgxYYnh"}},{"cell_type":"code","source":"avg_syllable_count = []\n\nfor fname in filenames:\n\n  file = open(fname, 'r')\n  text = file.read()\n  \n  sentence_list = text.split(\".\") #split function will return a list containting sentences seperated by full-stop\n  vowels = 'aeiou'\n\n  text = re.sub(r'\\.', '', text)\n  \n  text = text.split(\" \")\n\n  syl_count = 0  \n  \n  for word in text:\n    syllables = re.findall(f'(?!e$)(?!es$)(?!ed$)[{vowels}]', word, re.I)\n    syl_count += len(syllables)\n\n  avg_syllable_count.append(syl_count / len(text))\n  \n ","metadata":{"id":"SSJQnfA3YoU9","execution":{"iopub.status.busy":"2023-05-08T17:44:34.667789Z","iopub.execute_input":"2023-05-08T17:44:34.668081Z","iopub.status.idle":"2023-05-08T17:44:34.953656Z","shell.execute_reply.started":"2023-05-08T17:44:34.668056Z","shell.execute_reply":"2023-05-08T17:44:34.952278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Complex Word Count: Word containing two or more syllables","metadata":{"id":"HvNgBkdgX6bV"}},{"cell_type":"code","source":"complex_word_count = []\nword_count = []\n\nfor fname in filenames:\n\n  file = open(fname, 'r')\n  text = file.read()\n  \n  sentence_list = text.split(\".\") #split function will return a list containting sentences seperated by full-stop\n\n  text = re.sub(r'\\.', '', text)\n  \n  words_list = text.split(\" \")\n  word_count.append(len(words_list))\n\n  c = 0\n  for word in words_list:\n    l = re.findall('(?!e$)[aeiou]+', word, re.I)+re.findall('^[aeiouy]*e$', word, re.I)\n    if len(l) > 2:\n      c += 1\n  complex_word_count.append(c)\n","metadata":{"id":"9_P4beM_YJ4c","execution":{"iopub.status.busy":"2023-05-08T17:44:34.954616Z","iopub.execute_input":"2023-05-08T17:44:34.954977Z","iopub.status.idle":"2023-05-08T17:44:35.379719Z","shell.execute_reply.started":"2023-05-08T17:44:34.954947Z","shell.execute_reply":"2023-05-08T17:44:35.378513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Percentage of Complex words:\n (the number of complex words / the number of words) *100","metadata":{"id":"WkLvq1F7YPky"}},{"cell_type":"code","source":"complex_words_percent = []\nfor i in range(len(complex_word_count)):\n  complex_words_percent.append((complex_word_count[i]*100)/word_count[i])\n","metadata":{"id":"Kz-LAl65YXkt","execution":{"iopub.status.busy":"2023-05-08T17:44:35.381058Z","iopub.execute_input":"2023-05-08T17:44:35.381451Z","iopub.status.idle":"2023-05-08T17:44:35.386401Z","shell.execute_reply.started":"2023-05-08T17:44:35.381424Z","shell.execute_reply":"2023-05-08T17:44:35.38546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fog Index:\n0.4 * (Average Sentence Length + Percentage of Complex words)","metadata":{"id":"xPTM9rr8Yva3"}},{"cell_type":"code","source":"fog_index = []\n\nfor i in range(len(avg_sentence_len)):\n  fog_index.append((complex_words_percent[i] + avg_sentence_len[i])*0.4)\n","metadata":{"id":"K5uBKf0NY563","execution":{"iopub.status.busy":"2023-05-08T17:44:35.3873Z","iopub.execute_input":"2023-05-08T17:44:35.387569Z","iopub.status.idle":"2023-05-08T17:44:35.40245Z","shell.execute_reply.started":"2023-05-08T17:44:35.387545Z","shell.execute_reply":"2023-05-08T17:44:35.401245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Output DataFrame","metadata":{"id":"O34wxx92WijO"}},{"cell_type":"code","source":"output_data = input_data.copy()\n\noutput_data['Positive Score'] = pd.Series(positivity_score)\noutput_data['Negative Score'] = pd.Series(negativity_score)\noutput_data['Polarity Score'] = pd.Series(polarity_score)\noutput_data['Subjectivity Score'] = pd.Series(subjectivity_score)\n\noutput_data['Avg Sentence Length'] = pd.Series(avg_sentence_len)\noutput_data['Avg No of Words per Sentence'] = pd.Series(avg_words_per_sentence)\noutput_data['Avg Word Length'] = pd.Series(avg_words_length)\n\noutput_data['Personal Pronoun Count'] = pd.Series(pronoun_count)\noutput_data['Word Count'] = pd.Series(word_count)\noutput_data['Average Syllable Count'] = pd.Series(avg_syllable_count)\noutput_data['Complex Word Count'] = pd.Series(complex_word_count)\n\noutput_data['Percentage of Complex Words'] = pd.Series(complex_words_percent)\n\noutput_data['Fog Index'] = pd.Series(fog_index)\n\nprint(output_data)\n  \n","metadata":{"id":"3WA3f20GVVZh","outputId":"5b25c610-5fdf-4975-8408-3f920c373017","execution":{"iopub.status.busy":"2023-05-08T17:44:35.403446Z","iopub.execute_input":"2023-05-08T17:44:35.403775Z","iopub.status.idle":"2023-05-08T17:44:35.431715Z","shell.execute_reply.started":"2023-05-08T17:44:35.403747Z","shell.execute_reply":"2023-05-08T17:44:35.430738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Output data frame to excel sheet","metadata":{"id":"rVp8RIGcnxZi"}},{"cell_type":"code","source":"output_data.to_excel('Output Data Structure.xlsx', index = False)","metadata":{"id":"kq-B7V1Cnw2x","execution":{"iopub.status.busy":"2023-05-08T17:44:35.43296Z","iopub.execute_input":"2023-05-08T17:44:35.433355Z","iopub.status.idle":"2023-05-08T17:44:35.507021Z","shell.execute_reply.started":"2023-05-08T17:44:35.433326Z","shell.execute_reply":"2023-05-08T17:44:35.505792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_data.head(10)","metadata":{"id":"lbUMlloOoKd4","execution":{"iopub.status.busy":"2023-05-08T17:45:29.815574Z","iopub.execute_input":"2023-05-08T17:45:29.815955Z","iopub.status.idle":"2023-05-08T17:45:29.844906Z","shell.execute_reply.started":"2023-05-08T17:45:29.815926Z","shell.execute_reply":"2023-05-08T17:45:29.843749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}